# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LyF4ua4THjf3cRnv--ejyWmNMR5kSRux
"""



# Bayesian Neural Network Example Code

Building networks:import tensorflow as tf
import numpy as np
import pandas as pd
from tensorflow.keras.utils import to_categorical
import tensorflow_probability as tfp
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D,
Dropout
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.optimizers import RMSprop
from sklearn.metrics import classification_report # investigate class
recall
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import gc # clearing data
tfd = tfp.distributions
tfpl = tfp.layers

def nll(y_true, y_pred):
    return -y_pred.log_prob(y_true)

# The spike and slab distribution for variable selection. This technique
#allows for the use of prior knowledge. The spike represents the probability to model coefficients
#towards zero. The slab is the prior
# distribution for the coefficicent values for regression. Function from[1]

def spike_and_slab(event_shape, dtype):
    distribution = tfd.Mixture(
    cat=tfd.Categorical(probs=[0.5, 0.5]),
    components=[
    tfd.Independent(tfd.Normal(
    loc=tf.zeros(event_shape, dtype=dtype),
    scale=1.0*tf.ones(event_shape, dtype=dtype)),
    reinterpreted_batch_ndims=1),
    tfd.Independent(tfd.Normal(
    loc=tf.zeros(event_shape, dtype=dtype),
    scale=10.0*tf.ones(event_shape, dtype=dtype)),
    reinterpreted_batch_ndims=1)],
    name='spike_and_slab')
    return distribution

input_shape = (28, 28, 1) # bw image
divergence_fn = divergence_fn

layer1 = tfpl.Convolution2DReparameterization(
input_shape=input_shape, filters=8, kernel_size=(5, 5),
activation='relu', padding='VALID',
kernel_prior_fn=tfpl.default_multivariate_normal_fn, #
tensorflow defaults
kernel_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),
kernel_divergence_fn=divergence_fn,
bias_prior_fn=tfpl.default_multivariate_normal_fn,
bias_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),
bias_divergence_fn=divergence_fn)

layer2 = tfpl.Convolution2DReparameterization( # 2d COV layers as required
input_shape=input_shape, filters=8, kernel_size=(5, 5),
activation='relu', padding='VALID',
kernel_prior_fn=tfpl.default_multivariate_normal_fn

kernel_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),
kernel_divergence_fn=divergence_fn,
bias_prior_fn=tfpl.default_multivariate_normal_fn,
bias_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),
bias_divergence_fn=divergence_fn)

# acquire the prior distribution using the spike and slab

m = kernel_size+bias_size # different from class number

prior_model = Sequential([tfpl.DistributionLambda(lambda t :
spike_and_slab(n, dtype))])

# acquire posterior, creating variable layer

PV = Sequential([tfpl.VariableLayer(tfpl.IndependentNormal.params_size(n),
dtype=dtype),tfpl.IndependentNormal(n)])

# acquire dense variational layer where m is number of classes

kl_weight=1/X_train.shape[0] # using training data for initial knowledge
DVL = tfpl.DenseVariational(units=m, make_posterior_fn=posterior_model,
make_prior_fn=prior_model, kl_weight=kl_weight)

# set divergence
tf.random.set_seed(0)
divergence_fn = lambda q, p, _ : tfd.kl_divergence(q, p) / X_train.shape[0]

bayesian_model = Sequential([
layer1,
layer2,
MaxPooling2D(pool_size=(6, 6)),
Flatten(),
#Dropout here if required (0.5),
DVL,
tfpl.OneHotCategorical(m, convert_to_tensor_fn=tfd.Distribution.mode)
#OHE for classes
])
bayesian_model.compile(loss=nll, # null loss from previous model
optimizer=RMSprop(), # best performing optimizer
metrics=['accuracy'], # select metric
experimental_run_tf_function=False)
bayesian_model.fit(x_train, y_train_oh, epochs=5)